{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YmPtgQHTVhPl"
   },
   "source": [
    "# Preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dima/anaconda3/envs/dla/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lhJ98OWOcWFw",
    "outputId": "1e30ed81-33ff-4830-e913-8a3c0e177c6d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%bash\n",
    "\n",
    "# #download LjSpeech\n",
    "# wget https://data.keithito.com/data/speech/LJSpeech-1.1.tar.bz2 -o /dev/null\n",
    "# mkdir data\n",
    "# tar -xvf LJSpeech-1.1.tar.bz2 >> /dev/null\n",
    "# mv LJSpeech-1.1 data/LJSpeech-1.1\n",
    "\n",
    "# gdown https://drive.google.com/u/0/uc?id=1-EdH0t0loc6vPiuVtXdhsDtzygWNSNZx\n",
    "# mv train.txt data/\n",
    "\n",
    "# #download Waveglow\n",
    "# gdown https://drive.google.com/u/0/uc?id=1WsibBTsuRg_SF2Z6L6NFRTT-NjEy1oTx\n",
    "# mkdir -p waveglow/pretrained_model/\n",
    "# mv waveglow_256channels_ljs_v2.pt waveglow/pretrained_model/waveglow_256channels.pt\n",
    "\n",
    "# gdown https://drive.google.com/u/0/uc?id=1cJKJTmYd905a-9GFoo5gKjzhKjUVj83j\n",
    "# tar -xvf mel.tar.gz\n",
    "# echo $(ls mels | wc -l)\n",
    "\n",
    "# #download alignments\n",
    "# wget https://github.com/xcmyz/FastSpeech/raw/master/alignments.zip\n",
    "# unzip alignments.zip >> /dev/null\n",
    "\n",
    "# # we will use waveglow code, data and audio preprocessing from this repo\n",
    "# git clone https://github.com/xcmyz/FastSpeech.git\n",
    "# mv FastSpeech/text .\n",
    "# mv FastSpeech/audio .\n",
    "# mv FastSpeech/waveglow/* waveglow/\n",
    "# mv FastSpeech/utils.py .\n",
    "# mv FastSpeech/glow.py ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HxGYckguVhPv"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J9VQ0Rb-D4CQ",
    "outputId": "b56af2d0-df72-4458-abeb-09a2ba79092b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing, in the only sense with which we are at present concerned, differs from most if not from all the arts and crafts represented in the Exhibition\n",
      "in being comparatively modern.\n",
      "For although the Chinese took impressions from wood blocks engraved in relief for centuries before the woodcutters of the Netherlands, by a similar process\n",
      "produced the block books, which were the immediate predecessors of the true printed book,\n",
      "the invention of movable metal letters in the middle of the fifteenth century may justly be considered as the invention of the art of printing.\n"
     ]
    }
   ],
   "source": [
    "!head -n 5 data/train.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "BB0BfT7PVhPv"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1OLw43TwLwQI",
    "outputId": "239cabce-2939-4d8c-ae2d-fc103825005f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  4,  7,  4,  6, 18,  4,  9, 18,  4,  7,  3,  0,  3,  0,  7,  0,\n",
       "       16,  3,  4,  8,  0, 14, 12,  7,  2,  5,  0,  5,  6,  0,  4,  0,  6,\n",
       "        0,  8,  4,  2,  0,  7,  9,  0, 10,  0,  8,  0,  6,  3,  0,  7,  4,\n",
       "        8,  7,  3,  3,  2,  0,  7,  6,  5, 12,  9, 14,  4,  0,  6, 23, 16,\n",
       "        2, 10,  3,  3,  4, 12, 11, 12,  2,  2,  4,  5,  0,  8, 15,  9,  3,\n",
       "        0,  7,  7,  0,  8, 10,  5,  0,  6,  2,  5,  7,  9,  9,  5,  6,  0,\n",
       "        4,  0,  7,  0,  8,  8,  6,  6,  0,  2,  2,  4,  0,  6,  8, 16,  6,\n",
       "        6,  7,  0, 10,  8,  5,  3,  4,  6,  8,  2,  3,  7,  4,  6,  9,  6,\n",
       "        0,  0,  0,  7,  0,  8,  7,  8,  3,  6,  9,  5,  5,  7, 19],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.load('./alignments/0.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AQLnllbBPX1U",
    "outputId": "077d71c2-6338-4642-b436-4cc23e4ac357"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152\n"
     ]
    }
   ],
   "source": [
    "!head -n 1 data/train.txt | wc -m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f5Fczzy_VhPx"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "1XEo6WrJcXlE"
   },
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import random\n",
    "import itertools\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "from IPython import display\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import distributions\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchaudio\n",
    "from torchaudio.transforms import MelSpectrogram\n",
    "import math\n",
    "import time\n",
    "import os\n",
    "import librosa\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "from dataclasses import dataclass\n",
    "from collections import OrderedDict\n",
    "\n",
    "import seaborn as sns \n",
    "sns.set()\n",
    "\n",
    "import sys\n",
    "sys.path.append('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2atWw71EVhPz"
   },
   "source": [
    "# Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "R4JQ87YUVhPz"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class MelSpectrogramConfig:\n",
    "    num_mels = 80\n",
    "\n",
    "@dataclass\n",
    "class FastSpeechConfig:\n",
    "    vocab_size = 300\n",
    "    max_seq_len = 3000\n",
    "\n",
    "    encoder_dim = 256\n",
    "    encoder_n_layer = 4\n",
    "    encoder_head = 2\n",
    "    encoder_conv1d_filter_size = 1024\n",
    "\n",
    "    decoder_dim = 256\n",
    "    decoder_n_layer = 4\n",
    "    decoder_head = 2\n",
    "    decoder_conv1d_filter_size = 1024\n",
    "\n",
    "    fft_conv1d_kernel = (9, 1)\n",
    "    fft_conv1d_padding = (4, 0)\n",
    "\n",
    "    duration_predictor_filter_size = 256\n",
    "    duration_predictor_kernel_size = 3\n",
    "    dropout = 0.1\n",
    "    \n",
    "    PAD = 0\n",
    "    UNK = 1\n",
    "    BOS = 2\n",
    "    EOS = 3\n",
    "\n",
    "    PAD_WORD = '<blank>'\n",
    "    UNK_WORD = '<unk>'\n",
    "    BOS_WORD = '<s>'\n",
    "    EOS_WORD = '</s>'\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainConfig:\n",
    "    checkpoint_path = \"./model_new\"\n",
    "    logger_path = \"./logger\"\n",
    "    mel_ground_truth = \"./mels\"\n",
    "    alignment_path = \"./alignments\"\n",
    "    data_path = './data/train.txt'\n",
    "    \n",
    "    wandb_project = 'fastspeech_example'\n",
    "    \n",
    "    text_cleaners = ['english_cleaners']\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    device = 'cuda:0'\n",
    "\n",
    "    batch_size = 16\n",
    "    epochs = 2000\n",
    "    n_warm_up_step = 4000\n",
    "\n",
    "    learning_rate = 1e-3\n",
    "    weight_decay = 1e-6\n",
    "    grad_clip_thresh = 1.0\n",
    "    decay_step = [500000, 1000000, 2000000]\n",
    "\n",
    "    save_step = 3000\n",
    "    log_step = 5\n",
    "    clear_Time = 20\n",
    "\n",
    "    batch_expand_size = 32\n",
    "    \n",
    "\n",
    "mel_config = MelSpectrogramConfig()\n",
    "model_config = FastSpeechConfig()\n",
    "train_config = TrainConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "p_rS9iFUVhP0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dima/DLA/TTS/text/__init__.py:75: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  return s in _symbol_to_id and s is not '_' and s is not '~'\n",
      "/home/dima/DLA/TTS/text/__init__.py:75: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  return s in _symbol_to_id and s is not '_' and s is not '~'\n"
     ]
    }
   ],
   "source": [
    "from text import text_to_sequence\n",
    "\n",
    "\n",
    "def pad_1D(inputs, PAD=0):\n",
    "\n",
    "    def pad_data(x, length, PAD):\n",
    "        x_padded = np.pad(x, (0, length - x.shape[0]),\n",
    "                          mode='constant',\n",
    "                          constant_values=PAD)\n",
    "        return x_padded\n",
    "\n",
    "    max_len = max((len(x) for x in inputs))\n",
    "    padded = np.stack([pad_data(x, max_len, PAD) for x in inputs])\n",
    "\n",
    "    return padded\n",
    "\n",
    "\n",
    "def pad_1D_tensor(inputs, PAD=0):\n",
    "\n",
    "    def pad_data(x, length, PAD):\n",
    "        x_padded = F.pad(x, (0, length - x.shape[0]))\n",
    "        return x_padded\n",
    "\n",
    "    max_len = max((len(x) for x in inputs))\n",
    "    padded = torch.stack([pad_data(x, max_len, PAD) for x in inputs])\n",
    "\n",
    "    return padded\n",
    "\n",
    "\n",
    "def pad_2D(inputs, maxlen=None):\n",
    "\n",
    "    def pad(x, max_len):\n",
    "        PAD = 0\n",
    "        if np.shape(x)[0] > max_len:\n",
    "            raise ValueError(\"not max_len\")\n",
    "\n",
    "        s = np.shape(x)[1]\n",
    "        x_padded = np.pad(x, (0, max_len - np.shape(x)[0]),\n",
    "                          mode='constant',\n",
    "                          constant_values=PAD)\n",
    "        return x_padded[:, :s]\n",
    "\n",
    "    if maxlen:\n",
    "        output = np.stack([pad(x, maxlen) for x in inputs])\n",
    "    else:\n",
    "        max_len = max(np.shape(x)[0] for x in inputs)\n",
    "        output = np.stack([pad(x, max_len) for x in inputs])\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def pad_2D_tensor(inputs, maxlen=None):\n",
    "\n",
    "    def pad(x, max_len):\n",
    "        if x.size(0) > max_len:\n",
    "            raise ValueError(\"not max_len\")\n",
    "\n",
    "        s = x.size(1)\n",
    "        x_padded = F.pad(x, (0, 0, 0, max_len-x.size(0)))\n",
    "        return x_padded[:, :s]\n",
    "\n",
    "    if maxlen:\n",
    "        output = torch.stack([pad(x, maxlen) for x in inputs])\n",
    "    else:\n",
    "        max_len = max(x.size(0) for x in inputs)\n",
    "        output = torch.stack([pad(x, max_len) for x in inputs])\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def process_text(train_text_path):\n",
    "    with open(train_text_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        txt = []\n",
    "        for line in f.readlines():\n",
    "            txt.append(line)\n",
    "\n",
    "        return txt\n",
    "\n",
    "\n",
    "def get_data_to_buffer(train_config):\n",
    "    buffer = list()\n",
    "    text = process_text(train_config.data_path)\n",
    "\n",
    "    start = time.perf_counter()\n",
    "    for i in tqdm(range(len(text))):\n",
    "\n",
    "        mel_gt_name = os.path.join(\n",
    "            train_config.mel_ground_truth, \"ljspeech-mel-%05d.npy\" % (i+1))\n",
    "        mel_gt_target = np.load(mel_gt_name)\n",
    "        duration = np.load(os.path.join(\n",
    "            train_config.alignment_path, str(i)+\".npy\"))\n",
    "        character = text[i][0:len(text[i])-1]\n",
    "        character = np.array(\n",
    "            text_to_sequence(character, train_config.text_cleaners))\n",
    "\n",
    "        character = torch.from_numpy(character)\n",
    "        duration = torch.from_numpy(duration)\n",
    "        mel_gt_target = torch.from_numpy(mel_gt_target)\n",
    "\n",
    "        buffer.append({\"text\": character, \"duration\": duration,\n",
    "                       \"mel_target\": mel_gt_target})\n",
    "\n",
    "    end = time.perf_counter()\n",
    "    print(\"cost {:.2f}s to load all data into buffer.\".format(end-start))\n",
    "\n",
    "    return buffer\n",
    "\n",
    "\n",
    "class BufferDataset(Dataset):\n",
    "    def __init__(self, buffer):\n",
    "        self.buffer = buffer\n",
    "        self.length_dataset = len(self.buffer)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length_dataset\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.buffer[idx]\n",
    "\n",
    "\n",
    "def reprocess_tensor(batch, cut_list):\n",
    "    texts = [batch[ind][\"text\"] for ind in cut_list]\n",
    "    mel_targets = [batch[ind][\"mel_target\"] for ind in cut_list]\n",
    "    durations = [batch[ind][\"duration\"] for ind in cut_list]\n",
    "\n",
    "    length_text = np.array([])\n",
    "    for text in texts:\n",
    "        length_text = np.append(length_text, text.size(0))\n",
    "\n",
    "    src_pos = list()\n",
    "    max_len = int(max(length_text))\n",
    "    for length_src_row in length_text:\n",
    "        src_pos.append(np.pad([i+1 for i in range(int(length_src_row))],\n",
    "                              (0, max_len-int(length_src_row)), 'constant'))\n",
    "    src_pos = torch.from_numpy(np.array(src_pos))\n",
    "\n",
    "    length_mel = np.array(list())\n",
    "    for mel in mel_targets:\n",
    "        length_mel = np.append(length_mel, mel.size(0))\n",
    "\n",
    "    mel_pos = list()\n",
    "    max_mel_len = int(max(length_mel))\n",
    "    for length_mel_row in length_mel:\n",
    "        mel_pos.append(np.pad([i+1 for i in range(int(length_mel_row))],\n",
    "                              (0, max_mel_len-int(length_mel_row)), 'constant'))\n",
    "    mel_pos = torch.from_numpy(np.array(mel_pos))\n",
    "\n",
    "    texts = pad_1D_tensor(texts)\n",
    "    durations = pad_1D_tensor(durations)\n",
    "    mel_targets = pad_2D_tensor(mel_targets)\n",
    "\n",
    "    out = {\"text\": texts,\n",
    "           \"mel_target\": mel_targets,\n",
    "           \"duration\": durations,\n",
    "           \"mel_pos\": mel_pos,\n",
    "           \"src_pos\": src_pos,\n",
    "           \"mel_max_len\": max_mel_len}\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def collate_fn_tensor(batch):\n",
    "    len_arr = np.array([d[\"text\"].size(0) for d in batch])\n",
    "    index_arr = np.argsort(-len_arr)\n",
    "    batchsize = len(batch)\n",
    "    real_batchsize = batchsize // train_config.batch_expand_size\n",
    "\n",
    "    cut_list = list()\n",
    "    for i in range(train_config.batch_expand_size):\n",
    "        cut_list.append(index_arr[i*real_batchsize:(i+1)*real_batchsize])\n",
    "\n",
    "    output = list()\n",
    "    for i in range(train_config.batch_expand_size):\n",
    "        output.append(reprocess_tensor(batch, cut_list[i]))\n",
    "\n",
    "    return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZqbM8BM_VhP3",
    "outputId": "b19dbf62-aebe-4446-e8bd-5cef9a3d195f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13100/13100 [00:10<00:00, 1294.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost 10.12s to load all data into buffer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "buffer = get_data_to_buffer(train_config)\n",
    "\n",
    "dataset = BufferDataset(buffer)\n",
    "\n",
    "training_loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=train_config.batch_expand_size * train_config.batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn_tensor,\n",
    "    drop_last=True,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "4wHVq2vCVhP3"
   },
   "outputs": [],
   "source": [
    "batch = next(iter(training_loader))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CdMJ2fHNVhP4",
    "outputId": "97a475be-18a4-4b77-d902-846b1c228ac1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 173])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['text'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1wTLlW_oVhP4"
   },
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h2gKTZZ2VhP4"
   },
   "source": [
    "## Transformer Block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A6JW0f_XVhP4"
   },
   "source": [
    "### Scaled Dot Product Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8nBAyv2sVhP5"
   },
   "source": [
    "Here we have computer q,k,v matricies and we should calc attention score and calc weighted value vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "meWHUtTwVhP5"
   },
   "source": [
    "<img src=\"https://img-blog.csdnimg.cn/20190325121034288.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "4a_LVGUnVhP5"
   },
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    ''' Scaled Dot-Product Attention '''\n",
    "\n",
    "    def __init__(self, temperature, attn_dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.dropout = nn.Dropout(attn_dropout)\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        # q, k, v: [ (batch_size * n_heads) x seq_len x hidden_size ]\n",
    "        \n",
    "        attn = torch.einsum('BLD,BlD->BLl', q, k)\n",
    "        attn /= self.temperature\n",
    "\n",
    "        if mask is not None:\n",
    "            attn = torch.masked_fill(attn, mask, -torch.inf)\n",
    "\n",
    "        # attn: [ (batch_size * n_heads) x seq_len x seq_len ]\n",
    "        attn = self.dropout(self.softmax(attn))\n",
    "        output = torch.einsum('BLl,BlD->BLD', attn, v)\n",
    "\n",
    "        # output: [ (batch_size * n_heads) x seq_len x hidden_size ]\n",
    "        return output, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "2ksfrVLEVhP5"
   },
   "outputs": [],
   "source": [
    "dot_product_attention = ScaledDotProductAttention(1)\n",
    "\n",
    "q = torch.randn(4 * 4, 8, 4)\n",
    "k = torch.randn(4 * 4, 8, 4)\n",
    "v = torch.randn(4 * 4, 8, 4)\n",
    "\n",
    "output, attn = dot_product_attention(q, k, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TFCmFVxnVhP6"
   },
   "source": [
    "Посмотрим на аттэншн мапы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "vkUtTWbcVhP6"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2400x400 with 8 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(ncols=4, figsize=(24, 4))\n",
    "for i, at in enumerate(attn[0:4]):\n",
    "    sns.heatmap(attn[i], ax=ax[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2TENMqCIVhP6"
   },
   "source": [
    "### Multi Head Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sr8NhIX5VhP6"
   },
   "source": [
    "<img src='https://github.com/markovka17/dla/blob/2022/week07/images/attention_picture.png?raw=1'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c_u1jiAdVhP7"
   },
   "source": [
    "Тут инпуты тремя линейными сломи преобразуем для подачи в selt attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "uDtyFaj6VhP7"
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    ''' Multi-Head Attention module '''\n",
    "\n",
    "    def __init__(self, n_head, d_model, d_k, d_v, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_head = n_head\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.w_qs = nn.Linear(d_model, n_head * d_k)\n",
    "        self.w_ks = nn.Linear(d_model, n_head * d_k)\n",
    "        self.w_vs = nn.Linear(d_model, n_head * d_v)\n",
    "\n",
    "        self.attention = ScaledDotProductAttention(\n",
    "            temperature=d_k**0.5) \n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.fc = nn.Linear(n_head * d_v, d_model)\n",
    "        nn.init.xavier_normal_(self.fc.weight)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "         # normal distribution initialization better than kaiming(default in pytorch)\n",
    "        nn.init.normal_(self.w_qs.weight, mean=0,\n",
    "                        std=np.sqrt(2.0 / (self.d_model + self.d_k)))\n",
    "        nn.init.normal_(self.w_ks.weight, mean=0,\n",
    "                        std=np.sqrt(2.0 / (self.d_model + self.d_k)))\n",
    "        nn.init.normal_(self.w_vs.weight, mean=0,\n",
    "                        std=np.sqrt(2.0 / (self.d_model + self.d_v))) \n",
    "        \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        d_k, d_v, n_head = self.d_k, self.d_v, self.n_head\n",
    "\n",
    "        sz_b, len_q, _ = q.size()\n",
    "        sz_b, len_k, _ = k.size()\n",
    "        sz_b, len_v, _ = v.size()\n",
    "\n",
    "        residual = q\n",
    "\n",
    "        q = self.w_qs(q).view(sz_b, len_q, n_head, d_k)\n",
    "        k = self.w_ks(k).view(sz_b, len_k, n_head, d_k)\n",
    "        v = self.w_vs(v).view(sz_b, len_v, n_head, d_v)\n",
    "\n",
    "        q = q.permute(2, 0, 1, 3).contiguous().view(-1, len_q, d_k)  # (n*b) x lq x dk\n",
    "        k = k.permute(2, 0, 1, 3).contiguous().view(-1, len_k, d_k)  # (n*b) x lk x dk\n",
    "        v = v.permute(2, 0, 1, 3).contiguous().view(-1, len_v, d_v)  # (n*b) x lv x dv\n",
    "        \n",
    "        if mask is not None:\n",
    "            mask = mask.repeat(n_head, 1, 1)  # (n*b) x .. x ..\n",
    "        output, attn = self.attention(q, k, v, mask=mask)\n",
    "\n",
    "        output = output.view(n_head, sz_b, len_q, d_v)\n",
    "        output = output.permute(1, 2, 0, 3).contiguous().view(sz_b, len_q, -1)  # b x lq x (n*dv)\n",
    "\n",
    "        output = self.dropout(self.fc(output))\n",
    "        output = self.layer_norm(output + residual)\n",
    "\n",
    "        return output, attn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m9Vz6UBFVhP7"
   },
   "source": [
    "- Realization from Andrey Karpathy- https://github.com/karpathy/minGPT/blob/master/mingpt/model.py\n",
    "- Flash Attention - https://arxiv.org/abs/2205.14135"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cVgxfqV-VhP7"
   },
   "source": [
    "Двухслойная сеть из 1d конволюций с релу в качестве активации. После идет драпаут и layer norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EOZ29C1wVhP8"
   },
   "source": [
    "### Positionwise Feed Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "q8j0mGOpVhP8"
   },
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    ''' A two-feed-forward-layer module '''\n",
    "\n",
    "    def __init__(self, d_in, d_hid, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        # Use Conv1D\n",
    "        # position-wise\n",
    "        self.w_1 = nn.Conv1d(\n",
    "            d_in, d_hid, kernel_size=model_config.fft_conv1d_kernel[0], padding=model_config.fft_conv1d_padding[0])\n",
    "        # position-wise\n",
    "        self.w_2 = nn.Conv1d(\n",
    "            d_hid, d_in, kernel_size=model_config.fft_conv1d_kernel[1], padding=model_config.fft_conv1d_padding[1])\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm(d_in)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        output = x.transpose(1, 2)\n",
    "        output = self.w_2(F.relu(self.w_1(output)))\n",
    "        output = output.transpose(1, 2)\n",
    "        output = self.dropout(output)\n",
    "        output = self.layer_norm(output + residual)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JEEdzDaVVhP8"
   },
   "source": [
    "### FFTBlock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dYaMDsDlVhP8"
   },
   "source": [
    "<img src='https://github.com/markovka17/dla/blob/2022/week07/images/fft.png?raw=1'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iWZwoDWhVhP8"
   },
   "source": [
    "Совмещаем все вместе в один FFT(Feed Forward Transformer) BLock. Теперь можно стакать эти слои"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "5AWjgQMEVhP9"
   },
   "outputs": [],
   "source": [
    "class FFTBlock(torch.nn.Module):\n",
    "    \"\"\"FFT Block\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 d_model,\n",
    "                 d_inner,\n",
    "                 n_head,\n",
    "                 d_k,\n",
    "                 d_v,\n",
    "                 dropout=0.1):\n",
    "        super(FFTBlock, self).__init__()\n",
    "        self.slf_attn = MultiHeadAttention(\n",
    "            n_head, d_model, d_k, d_v, dropout=dropout)\n",
    "        self.pos_ffn = PositionwiseFeedForward(\n",
    "            d_model, d_inner, dropout=dropout)\n",
    "\n",
    "    def forward(self, enc_input, non_pad_mask=None, slf_attn_mask=None):\n",
    "        enc_output, enc_slf_attn = self.slf_attn(\n",
    "            enc_input, enc_input, enc_input, mask=slf_attn_mask)\n",
    "        \n",
    "        if non_pad_mask is not None:\n",
    "            enc_output *= non_pad_mask\n",
    "\n",
    "        enc_output = self.pos_ffn(enc_output)\n",
    "        \n",
    "        if non_pad_mask is not None:\n",
    "            enc_output *= non_pad_mask\n",
    "\n",
    "        return enc_output, enc_slf_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "tM2YNug-VhP9"
   },
   "outputs": [],
   "source": [
    "hidden_size = 16\n",
    "intermediate_size = 64\n",
    "n_head = 4\n",
    "batch_size = 4\n",
    "seq_len = 12\n",
    "\n",
    "fft_block = FFTBlock(hidden_size, intermediate_size, n_head, hidden_size // n_head, hidden_size // n_head)\n",
    "\n",
    "inp_tensor = torch.rand(batch_size, seq_len, hidden_size, dtype=torch.float32)\n",
    "\n",
    "out_tensor = fft_block(inp_tensor)[0]\n",
    "\n",
    "assert inp_tensor.shape == out_tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t7SApmHeVhP9"
   },
   "source": [
    "- Training tips - https://arxiv.org/pdf/1804.00247.pdf\n",
    "- Transformer without Tears - https://tnq177.github.io/data/transformers_without_tears.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4j1WQtx7VhP9"
   },
   "source": [
    "## Length Regulator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f486F3tsVhP9"
   },
   "source": [
    "### Aligner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "9YVe6gsKVhP9"
   },
   "outputs": [],
   "source": [
    "def create_alignment(base_mat, duration_predictor_output):\n",
    "    N, L = duration_predictor_output.shape\n",
    "    for i in range(N):\n",
    "        count = 0\n",
    "        for j in range(L):\n",
    "            for k in range(duration_predictor_output[i][j]):\n",
    "                base_mat[i][count+k][j] = 1\n",
    "            count = count + duration_predictor_output[i][j]\n",
    "    return base_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sxfLUTOXVhP-"
   },
   "source": [
    "функция которая делает бинарную матрицу для деблирование каждого мела"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "zowTKF9tVhP-"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.]]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_alignment(\n",
    "    torch.zeros(1, 6, 3).numpy(),\n",
    "    torch.LongTensor([[1,2,3]])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4H555SmSVhP-"
   },
   "source": [
    "### Duration Predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xVNEYVEzVhP-"
   },
   "source": [
    "<img src='https://github.com/markovka17/dla/blob/2022/week07/images/duration_predictor.png?raw=1'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "1gm1u8rdVhP-"
   },
   "outputs": [],
   "source": [
    "class Transpose(nn.Module):\n",
    "    def __init__(self, dim_1, dim_2):\n",
    "        super().__init__()\n",
    "        self.dim_1 = dim_1\n",
    "        self.dim_2 = dim_2\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.transpose(self.dim_1, self.dim_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2PmqEdxCVhP-"
   },
   "source": [
    "Простая сетка с двумя конволюшналами и Линейным слоем агрегатом. Предсказываем тут длительность каждой фонемы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "ZcEwNMulVhP_"
   },
   "outputs": [],
   "source": [
    "class DurationPredictor(nn.Module):\n",
    "    \"\"\" Duration Predictor \"\"\"\n",
    "\n",
    "    def __init__(self, model_config: FastSpeechConfig):\n",
    "        super(DurationPredictor, self).__init__()\n",
    "\n",
    "        self.input_size = model_config.encoder_dim\n",
    "        self.filter_size = model_config.duration_predictor_filter_size\n",
    "        self.kernel = model_config.duration_predictor_kernel_size\n",
    "        self.conv_output_size = model_config.duration_predictor_filter_size\n",
    "        self.dropout = model_config.dropout\n",
    "\n",
    "        self.conv_net = nn.Sequential(\n",
    "            Transpose(-1, -2),\n",
    "            nn.Conv1d(\n",
    "                self.input_size, self.filter_size,\n",
    "                kernel_size=self.kernel, padding=1\n",
    "            ),\n",
    "            Transpose(-1, -2),\n",
    "            nn.LayerNorm(self.filter_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(self.dropout),\n",
    "            Transpose(-1, -2),\n",
    "            nn.Conv1d(\n",
    "                self.filter_size, self.filter_size,\n",
    "                kernel_size=self.kernel, padding=1\n",
    "            ),\n",
    "            Transpose(-1, -2),\n",
    "            nn.LayerNorm(self.filter_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(self.dropout)\n",
    "        )\n",
    "\n",
    "        self.linear_layer = nn.Linear(self.conv_output_size, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, encoder_output):\n",
    "        encoder_output = self.conv_net(encoder_output)\n",
    "            \n",
    "        out = self.linear_layer(encoder_output)\n",
    "        out = self.relu(out)\n",
    "        out = out.squeeze()\n",
    "        if not self.training:\n",
    "            out = out.unsqueeze(0)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "jc1wvcYzVhP_"
   },
   "outputs": [],
   "source": [
    "dur_predictor = DurationPredictor(model_config)\n",
    "\n",
    "inp_tensor = torch.rand(\n",
    "    2, # batch_size\n",
    "    12, #seq_len\n",
    "    model_config.encoder_dim,\n",
    "    dtype=torch.float32\n",
    ")\n",
    "\n",
    "dur_prediction = dur_predictor(inp_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "f-SIGduUVhP_"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2786, 0.2115, 0.3364, 0.2446, 0.3571, 0.3957, 0.0000, 0.2793, 0.3473,\n",
       "         0.0965, 0.0354, 0.1384],\n",
       "        [0.0000, 0.5672, 0.0000, 0.0000, 0.0239, 0.3439, 0.1991, 0.4109, 0.2195,\n",
       "         0.4215, 0.1083, 0.2145]], grad_fn=<SqueezeBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dur_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "TgZh6eK4VhP_"
   },
   "outputs": [],
   "source": [
    "class LengthRegulator(nn.Module):\n",
    "    \"\"\" Length Regulator \"\"\"\n",
    "\n",
    "    def __init__(self, model_config):\n",
    "        super(LengthRegulator, self).__init__()\n",
    "        self.duration_predictor = DurationPredictor(model_config)\n",
    "\n",
    "    def LR(self, x, duration_predictor_output, mel_max_length=None):\n",
    "        expand_max_len = torch.max(\n",
    "            torch.sum(duration_predictor_output, -1), -1)[0]\n",
    "        alignment = torch.zeros(duration_predictor_output.size(0),\n",
    "                                expand_max_len,\n",
    "                                duration_predictor_output.size(1)).numpy()\n",
    "        alignment = create_alignment(alignment,\n",
    "                                     duration_predictor_output.cpu().numpy())\n",
    "        alignment = torch.from_numpy(alignment).to(x.device)\n",
    "\n",
    "        output = alignment @ x\n",
    "        if mel_max_length:\n",
    "            output = F.pad(\n",
    "                output, (0, 0, 0, mel_max_length-output.size(1), 0, 0))\n",
    "        return output\n",
    "\n",
    "    def forward(self, x, alpha=1.0, target=None, mel_max_length=None):\n",
    "        ### Your code here\n",
    "        duration_predictor_output = self.duration_predictor(x)\n",
    "\n",
    "        if target is not None:\n",
    "            output = self.LR(x, target, mel_max_length)\n",
    "            return output, duration_predictor_output\n",
    "        else:\n",
    "            output = self.LR(x, (duration_predictor_output * alpha + 0.5).int(), mel_max_length)\n",
    "            return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VAsXnAimVhP_"
   },
   "source": [
    "Класс который все объеденяет"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jje2Ssr_VhQA"
   },
   "source": [
    "## Final BLock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bjd0V3ZkVhQA"
   },
   "source": [
    "### Attention masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "0GOu96VeVhQA"
   },
   "outputs": [],
   "source": [
    "def get_non_pad_mask(seq):\n",
    "    assert seq.dim() == 2\n",
    "    return seq.ne(model_config.PAD).type(torch.float).unsqueeze(-1)\n",
    "\n",
    "def get_attn_key_pad_mask(seq_k, seq_q):\n",
    "    ''' For masking out the padding part of key sequence. '''\n",
    "    # Expand to fit the shape of key query attention matrix.\n",
    "    len_q = seq_q.size(1)\n",
    "    padding_mask = seq_k.eq(model_config.PAD)\n",
    "    padding_mask = padding_mask.unsqueeze(\n",
    "        1).expand(-1, len_q, -1)  # b x lq x lk\n",
    "\n",
    "    return padding_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LAQpCcPLVhQA"
   },
   "source": [
    "Энкодер и Декодер, берем FFT слои и цикликом по ним проходим. В энкодере эмбединги токенов и позишн эмбединги"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ssOhiRwQVhQA"
   },
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "PNUBCHrTVhQA"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, model_config):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        len_max_seq=model_config.max_seq_len\n",
    "        n_position = len_max_seq + 1\n",
    "        n_layers = model_config.encoder_n_layer\n",
    "\n",
    "        self.src_word_emb = nn.Embedding(\n",
    "            model_config.vocab_size,\n",
    "            model_config.encoder_dim,\n",
    "            padding_idx=model_config.PAD\n",
    "        )\n",
    "\n",
    "        self.position_enc = nn.Embedding(\n",
    "            n_position,\n",
    "            model_config.encoder_dim,\n",
    "            padding_idx=model_config.PAD\n",
    "        )\n",
    "\n",
    "        self.layer_stack = nn.ModuleList([FFTBlock(\n",
    "            model_config.encoder_dim,\n",
    "            model_config.encoder_conv1d_filter_size,\n",
    "            model_config.encoder_head,\n",
    "            model_config.encoder_dim // model_config.encoder_head,\n",
    "            model_config.encoder_dim // model_config.encoder_head,\n",
    "            dropout=model_config.dropout\n",
    "        ) for _ in range(n_layers)])\n",
    "\n",
    "    def forward(self, src_seq, src_pos, return_attns=False):\n",
    "\n",
    "        enc_slf_attn_list = []\n",
    "\n",
    "        # -- Prepare masks\n",
    "        slf_attn_mask = get_attn_key_pad_mask(seq_k=src_seq, seq_q=src_seq)\n",
    "        non_pad_mask = get_non_pad_mask(src_seq)\n",
    "        \n",
    "        # -- Forward\n",
    "        enc_output = self.src_word_emb(src_seq) + self.position_enc(src_pos)\n",
    "\n",
    "        for enc_layer in self.layer_stack:\n",
    "            enc_output, enc_slf_attn = enc_layer(\n",
    "                enc_output,\n",
    "                non_pad_mask=non_pad_mask,\n",
    "                slf_attn_mask=slf_attn_mask)\n",
    "            if return_attns:\n",
    "                enc_slf_attn_list += [enc_slf_attn]\n",
    "        \n",
    "\n",
    "        return enc_output, non_pad_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "Ab-JzxgzVhQB"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\" Decoder \"\"\"\n",
    "\n",
    "    def __init__(self, model_config):\n",
    "\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        len_max_seq=model_config.max_seq_len\n",
    "        n_position = len_max_seq + 1\n",
    "        n_layers = model_config.decoder_n_layer\n",
    "\n",
    "        self.position_enc = nn.Embedding(\n",
    "            n_position,\n",
    "            model_config.encoder_dim,\n",
    "            padding_idx=model_config.PAD,\n",
    "        )\n",
    "\n",
    "        self.layer_stack = nn.ModuleList([FFTBlock(\n",
    "            model_config.encoder_dim,\n",
    "            model_config.encoder_conv1d_filter_size,\n",
    "            model_config.encoder_head,\n",
    "            model_config.encoder_dim // model_config.encoder_head,\n",
    "            model_config.encoder_dim // model_config.encoder_head,\n",
    "            dropout=model_config.dropout\n",
    "        ) for _ in range(n_layers)])\n",
    "\n",
    "    def forward(self, enc_seq, enc_pos, return_attns=False):\n",
    "\n",
    "        dec_slf_attn_list = []\n",
    "\n",
    "        # -- Prepare masks\n",
    "        slf_attn_mask = get_attn_key_pad_mask(seq_k=enc_pos, seq_q=enc_pos)\n",
    "        non_pad_mask = get_non_pad_mask(enc_pos)\n",
    "\n",
    "        # -- Forward\n",
    "        dec_output = enc_seq + self.position_enc(enc_pos)\n",
    "\n",
    "        for dec_layer in self.layer_stack:\n",
    "            dec_output, dec_slf_attn = dec_layer(\n",
    "                dec_output,\n",
    "                non_pad_mask=non_pad_mask,\n",
    "                slf_attn_mask=slf_attn_mask)\n",
    "            if return_attns:\n",
    "                dec_slf_attn_list += [dec_slf_attn]\n",
    "\n",
    "        return dec_output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "DBG8cgpDVhQB"
   },
   "outputs": [],
   "source": [
    "def get_mask_from_lengths(lengths, max_len=None):\n",
    "    if max_len == None:\n",
    "        max_len = torch.max(lengths).item()\n",
    "\n",
    "    ids = torch.arange(0, max_len, 1, device=lengths.device)\n",
    "    mask = (ids < lengths.unsqueeze(1)).bool()\n",
    "\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "bXlC5JKmVhQB"
   },
   "outputs": [],
   "source": [
    "class FastSpeech(nn.Module):\n",
    "    \"\"\" FastSpeech \"\"\"\n",
    "\n",
    "    def __init__(self, model_config):\n",
    "        super(FastSpeech, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(model_config)\n",
    "        self.length_regulator = LengthRegulator(model_config)\n",
    "        self.decoder = Decoder(model_config)\n",
    "\n",
    "        self.mel_linear = nn.Linear(model_config.decoder_dim, mel_config.num_mels)\n",
    "\n",
    "    def mask_tensor(self, mel_output, position, mel_max_length):\n",
    "        lengths = torch.max(position, -1)[0]\n",
    "        mask = ~get_mask_from_lengths(lengths, max_len=mel_max_length)\n",
    "        mask = mask.unsqueeze(-1).expand(-1, -1, mel_output.size(-1))\n",
    "        return mel_output.masked_fill(mask, 0.)\n",
    "\n",
    "    def forward(self, src_seq, src_pos, mel_pos=None, mel_max_length=None, length_target=None, alpha=1.0):\n",
    "        encoder_out = self.encoder(src_seq, src_pos)\n",
    "        if self.training:\n",
    "            lr_output, duration_predictor_output = self.length_regulator(encoder_out, alpha, length_target, mel_max_length)\n",
    "            output = self.decoder(lr_output, mel_pos)\n",
    "            output = self.mask_tensor(output, mel_pos, mel_max_length)\n",
    "            output = self.mel_linear(output)\n",
    "            return output, duration_predictor_output\n",
    "        \n",
    "        lr_output, mel_pos = self.length_regulator(encoder_out, alpha, length_target, mel_max_length)\n",
    "        output = self.decoder(lr_output, mel_pos)\n",
    "        output = self.mel_linear(output)\n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D5Y5lIcBVhQB"
   },
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "Y0FIQ8eiVhQC"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class FastSpeechLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        self.l1_loss = nn.L1Loss()\n",
    "\n",
    "    def forward(self, mel, duration_predicted, mel_target, duration_predictor_target):\n",
    "        mel_loss = self.mse_loss(mel, mel_target)\n",
    "\n",
    "        duration_predictor_loss = self.l1_loss(duration_predicted,\n",
    "                                               duration_predictor_target.float())\n",
    "\n",
    "        return mel_loss, duration_predictor_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TEQkWzd6VhQC"
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "3zRoUHYdVhQC"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wandb_writer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [33], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moptim\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlr_scheduler\u001b[39;00m  \u001b[39mimport\u001b[39;00m OneCycleLR\n\u001b[0;32m----> 2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mwandb_writer\u001b[39;00m \u001b[39mimport\u001b[39;00m WanDBWriter\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wandb_writer'"
     ]
    }
   ],
   "source": [
    "from torch.optim.lr_scheduler  import OneCycleLR\n",
    "from wandb_writer import WanDBWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "vAzbFhhyVhQC"
   },
   "outputs": [],
   "source": [
    "model = FastSpeech(model_config)\n",
    "model = model.to(train_config.device)\n",
    "\n",
    "fastspeech_loss = FastSpeechLoss()\n",
    "current_step = 0\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=train_config.learning_rate,\n",
    "    betas=(0.9, 0.98),\n",
    "    eps=1e-9)\n",
    "\n",
    "scheduler = OneCycleLR(optimizer, **{\n",
    "    \"steps_per_epoch\": len(training_loader) * train_config.batch_expand_size,\n",
    "    \"epochs\": train_config.epochs,\n",
    "    \"anneal_strategy\": \"cos\",\n",
    "    \"max_lr\": train_config.learning_rate,\n",
    "    \"pct_start\": 0.1\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Y6ogUmwVhQC"
   },
   "outputs": [],
   "source": [
    "logger = WanDBWriter(train_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sOwh9autVhQC",
    "tags": []
   },
   "source": [
    "## Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P9jGFiM8VhQC",
    "tags": []
   },
   "outputs": [],
   "source": [
    "tqdm_bar = tqdm(total=train_config.epochs * len(training_loader) * train_config.batch_expand_size - current_step)\n",
    "\n",
    "\n",
    "for epoch in range(train_config.epochs):\n",
    "    for i, batchs in enumerate(training_loader):\n",
    "        # real batch start here\n",
    "        for j, db in enumerate(batchs):\n",
    "            current_step += 1\n",
    "            tqdm_bar.update(1)\n",
    "            \n",
    "            # logger.set_step(current_step)\n",
    "\n",
    "            # Get Data\n",
    "            character = db[\"text\"].long().to(train_config.device)\n",
    "            mel_target = db[\"mel_target\"].float().to(train_config.device)\n",
    "            duration = db[\"duration\"].int().to(train_config.device)\n",
    "            mel_pos = db[\"mel_pos\"].long().to(train_config.device)\n",
    "            src_pos = db[\"src_pos\"].long().to(train_config.device)\n",
    "            max_mel_len = db[\"mel_max_len\"]\n",
    "\n",
    "            # Forward\n",
    "            mel_output, duration_predictor_output = model(character,\n",
    "                                                          src_pos,\n",
    "                                                          mel_pos=mel_pos,\n",
    "                                                          mel_max_length=max_mel_len,\n",
    "                                                          length_target=duration)\n",
    "\n",
    "            # Calc Loss\n",
    "            mel_loss, duration_loss = fastspeech_loss(mel_output,\n",
    "                                                    duration_predictor_output,\n",
    "                                                    mel_target,\n",
    "                                                    duration)\n",
    "            total_loss = mel_loss + duration_loss\n",
    "\n",
    "            # Logger\n",
    "            t_l = total_loss.detach().cpu().numpy()\n",
    "            m_l = mel_loss.detach().cpu().numpy()\n",
    "            d_l = duration_loss.detach().cpu().numpy()\n",
    "\n",
    "            # logger.add_scalar(\"duration_loss\", d_l)\n",
    "            # logger.add_scalar(\"mel_loss\", m_l)\n",
    "            # logger.add_scalar(\"total_loss\", t_l)\n",
    "\n",
    "            # Backward\n",
    "            total_loss.backward()\n",
    "\n",
    "            # Clipping gradients to avoid gradient explosion\n",
    "            nn.utils.clip_grad_norm_(\n",
    "                model.parameters(), train_config.grad_clip_thresh)\n",
    "            \n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            scheduler.step()\n",
    "\n",
    "            if current_step % train_config.save_step == 0:\n",
    "                torch.save({'model': model.state_dict(), 'optimizer': optimizer.state_dict(\n",
    "                )}, os.path.join(train_config.checkpoint_path, 'checkpoint_%d.pth.tar' % current_step))\n",
    "                print(\"save model at step %d ...\" % current_step)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b0u9XD4JVhQD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M2sFL4hCVhQD"
   },
   "outputs": [],
   "source": [
    "import waveglow\n",
    "import text\n",
    "import audio\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PJ55GSUIVhQD"
   },
   "source": [
    "Преобразовывать мел-спектрограмы в wav будем используя WaveGlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-vQhCPaGVhQD"
   },
   "outputs": [],
   "source": [
    "WaveGlow = utils.get_WaveGlow()\n",
    "WaveGlow = WaveGlow.cuda()\n",
    "\n",
    "model.load_state_dict(torch.load('../model_new/checkpoint_225000.pth.tar', map_location='cuda:0')['model'])\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nEyURXceVhQD"
   },
   "outputs": [],
   "source": [
    "def synthesis(model, text, alpha=1.0):\n",
    "    text = np.array(phn)\n",
    "    text = np.stack([text])\n",
    "    src_pos = np.array([i+1 for i in range(text.shape[1])])\n",
    "    src_pos = np.stack([src_pos])\n",
    "    sequence = torch.from_numpy(text).long().to(train_config.device)\n",
    "    src_pos = torch.from_numpy(src_pos).long().to(train_config.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        mel = model.forward(sequence, src_pos, alpha=alpha)\n",
    "    return mel[0].cpu().transpose(0, 1), mel.contiguous().transpose(1, 2)\n",
    "\n",
    "\n",
    "def get_data():\n",
    "    tests = [ \n",
    "        \"I am very happy to see you again!\",\n",
    "        \"Durian model is a very good speech synthesis!\",\n",
    "        \"When I was twenty, I fell in love with a girl.\",\n",
    "        \"I remove attention module in decoder and use average pooling to implement predicting r frames at once\",\n",
    "        \"You can not improve your past, but you can improve your future. Once time is wasted, life is wasted.\",\n",
    "        \"Death comes to all, but great achievements raise a monument which shall endure until the sun grows old.\"\n",
    "    ]\n",
    "    data_list = list(text.text_to_sequence(test, train_config.text_cleaners) for test in tests)\n",
    "\n",
    "    return data_list\n",
    "\n",
    "data_list = get_data()\n",
    "for speed in [0.8, 1., 1.3]:\n",
    "    for i, phn in tqdm(enumerate(data_list)):\n",
    "        mel, mel_cuda = synthesis(model, phn, speed)\n",
    "        \n",
    "        os.makedirs(\"results\", exist_ok=True)\n",
    "        \n",
    "        audio.tools.inv_mel_spec(\n",
    "            mel, f\"results/s={speed}_{i}.wav\"\n",
    "        )\n",
    "        \n",
    "        waveglow.inference.inference(\n",
    "            mel_cuda, WaveGlow,\n",
    "            f\"results/s={speed}_{i}_waveglow.wav\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yh0IVOoMVhQE"
   },
   "source": [
    "Сгенерим звук с тремя разными скоростями используя возможности фастспича"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xVhGsMqAVhQE"
   },
   "outputs": [],
   "source": [
    "display.Audio(\"results/s=0.8_5_waveglow.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pO72cZOHVhQE"
   },
   "outputs": [],
   "source": [
    "display.Audio(\"results/s=1.0_5_waveglow.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SVeJw23_VhQE"
   },
   "outputs": [],
   "source": [
    "display.Audio(\"results/s=1.3_5_waveglow.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JferieDTVhQE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aZwCKTWhVhQE"
   },
   "outputs": [],
   "source": [
    "display.Audio(\"results/s=0.8_5.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D3b-3fWDVhQE"
   },
   "outputs": [],
   "source": [
    "display.Audio(\"results/s=1.0_5.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZHVDGrsJVhQF"
   },
   "outputs": [],
   "source": [
    "display.Audio(\"results/s=1.3_5.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QhObSyB_VhQF"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3.10.8 ('dla')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "vscode": {
   "interpreter": {
    "hash": "ea1f423a9966528cc9303671e9fb78d67f4cd084e805716a038a20435527caf5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
